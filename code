 import torch
 import torchvision.transforms as transforms
 from torchvision.datasets import ImageFolder
 from torch.utils.data import DataLoader, random_split, SubsetRandomSampler,ConatDataset
 import numpy as np
 import torch
 import timm
 import torchsummary
 import torch
 import torch.nn as nn
 import torch.optim as optim
 from tqdm import tqdm
 from sklearn.metrics import confusion_matrix, classification_report, precision_score,,mrecall_score
 import matplotlib.pyplot as plt
 import seaborn as sns
 import numpy as np
 import os
 train_transform_augmented = transforms.Compose([
 transforms.Resize((224, 224)),
 transforms.RandomHorizontalFlip(),
 transforms.RandomRotation(10),
 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
 ])
 train_transform_no_augmentation = transforms.Compose([
 transforms.Resize((224, 224)),
 transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
 ])
 base_dir = '/content/HumanActionRecognition/dataset'
 full_dataset = ImageFolder(root=base_dir, transform=train_transform_no_augmentation)
 train_size = int(0.8 * len(full_dataset))
 val_size = len(full_dataset)- train_size
 train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
 target_samples_per_class =2500
class_indices = {class_idx: [] for class_idx in range(len(full_dataset.classes))}
 for idx, (_, label_idx) in enumerate(full_dataset.samples):
 class_indices[label_idx].append(idx)
 class_counts = np.array([len(class_indices[i]) for i in range(len(full_dataset.classes))])
 oversample_factors = np.maximum(0, (target_samples_per_class- class_counts)
 oversample_factors = np.floor(oversample_factors).astype(int)
 augmented_datasets = []
 for class_idx, factor in enumerate(oversample_factors):
 if factor > 0:
 class_subset = SubsetRandomSampler(class_indices[class_idx])
 augmented_dataset = ImageFolder(root=base_dir, transform=train_transform_augmented)
 augmented_datasets.append(augmented_dataset)
 datasets_to_combine = [train_dataset.dataset] + augmented_datasets
 combined_dataset = ConcatDataset(datasets_to_combine)
 batch_size = 16
 train_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)
 valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
 class_names = full_dataset.classes
 print("\nNumber of classes:", len(class_names))
 print("Class names:")
 for name in class_names:
 print(name)
 Number of classes: 15
 Class names:
 calling
 clapping
 cycling
 dancing
 drinking
 eating
 fighting
 hugging
 laughing
 listening_to_music
 running
 sitting
 sleeping
 texting
 using_laptop
 import matplotlib.pyplot as plt
 import numpy as np
 def visualize_samples_per_class(dataset, class_names, num_samples=3): """
 Function to visualize random sample images for each class."""
 fig,
 axes = plt.subplots(len(class_names), num_samples, figsize=(num_samples * 3,
 len(class_names) * 3))
for class_idx, class_name in enumerate(class_names):
 class_indices = [i for i, (_, label) in enumerate(dataset.samples) if label
 == class_idx]
 random_samples = np.random.choice(class_indices, num_samples, replace=False)
 for i, sample_idx in enumerate(random_samples):
 image, label = dataset[sample_idx]
 image = image.permute(1, 2, 0).numpy()
 image = np.clip(image * np.array([0.229, 0.224, 0.225]) +
 np.array([0.485, 0.456,
 0.406]), 0, 1)
 axes[class_idx, i].imshow(image)
 axes[class_idx, i].axis("off")
 if i == 0:
 axes[class_idx, i].set_title(f'{class_name}', fontsize=14)
 plt.tight_layout()
 plt.show()
 visualize_samples_per_class(full_dataset, class_names)
 def visualize_class_distribution(dataset, class_names):
 """
 Function to visualize the class distribution in the dataset.
 """
 class_counts = np.zeros(len(class_names))
 for _, label in dataset.samples:
 class_counts[label] += 1
 plt.figure(figsize=(10, 5))
 plt.bar(class_names, class_counts, color='skyblue')
 plt.xticks(rotation=90)
 plt.xlabel('Class')
 plt.ylabel('Number of Images')
 plt.title('Class Distribution in Dataset')
 plt.show()
 visualize_class_distribution(full_dataset, class_names)
 from PIL import Image
 def plot_image_size_distribution(dataset):
 """Plot the distribution of image sizes (width, height)."""
 widths = []
 heights = []
 for img_path, _ in dataset.samples:
 img = Image.open(img_path)
 width, height = img.size
 widths.append(width)
 heights.append(height)
 plt.figure(figsize=(10, 5))
 plt.subplot(1, 2, 1)
 plt.hist(widths, bins=20, color='skyblue')
plt.title("Distribution of Image Widths")
 plt.xlabel("Width")
 plt.ylabel("Frequency")
 plt.subplot(1, 2, 2)
 plt.hist(heights, bins=20, color='lightcoral')
 plt.title("Distribution of Image Heights")
 plt.xlabel("Height")
 plt.ylabel("Frequency")
 plt.tight_layout()
 plt.show()
 plot_image_size_distribution(full_dataset)
 def plot_aspect_ratio_distribution(dataset):
 """Plot the distribution of aspect ratios (width/height)"""
 aspect_ratios = []
 for img_path, _ in dataset.samples:
 img = Image.open(img_path)
 width, height = img.size
 aspect_ratio = width / height
 aspect_ratios.append(aspect_ratio)
 plt.figure(figsize=(7, 5))
 plt.hist(aspect_ratios, bins=20, color='lightgreen')
 plt.title("Aspect Ratio Distribution")
 plt.xlabel("Aspect Ratio (Width / Height)")
 plt.ylabel("Frequency")
 plt.show()
 plot_aspect_ratio_distribution(full_dataset)
 def plot_class_distribution_pie_chart(dataset, class_names):
 """Plot a pie chart to visualize class distribution."""
 class_counts = np.zeros(len(class_names))
 for _, label in dataset.samples:
 class_counts[label] += 1
 plt.figure(figsize=(8, 8))
 plt.pie(class_counts, labels=class_names, autopct='%1.1f%%',
 colors=plt.cm.Paired.colors,
 startangle=90)
 plt.axis('equal')
 plt.title('Class Distribution')
 plt.show()
 plot_class_distribution_pie_chart(full_dataset, class_names)
 def plot_image_size_vs_aspect_ratio(dataset):
 """Plot a scatter plot of image sizes (width x height) vs aspect ratios."""
 widths = []
heights = []
 aspect_ratios = []
 for img_path, _ in dataset.samples:
 img = Image.open(img_path)
 width, height = img.size
 aspect_ratios.append(width / height)
 widths.append(width)
 heights.append(height)
 plt.figure(figsize=(8, 6))
 plt.scatter(widths, aspect_ratios, color='teal', alpha=0.5)
 plt.xlabel('Image Width')
 plt.ylabel('Aspect Ratio (Width / Height)')
 plt.title('Image Size vs Aspect Ratio')
 plt.show()
 # Plot image size vs aspect ratio
 plot_image_size_vs_aspect_ratio(full_dataset)

  /***TRAININGCODE***/
 from flask import Flask, request, render_template, jsonify
 import torch
 import torch.nn.functional as F
 import torchvision.transforms as transforms
 from PIL import Image
 import numpy as np
 import base64
 from io import BytesIO
 import timm
 import cv2
 app = Flask(__name__)
 # Define class names
 class_names = [
 'calling', 'clapping', 'cycling', 'dancing', 'drinking', 'eating',
 'fighting', 'hugging', 'laughing', 'listening_to_music', 'running',
 'sitting', 'sleeping', 'texting', 'using_laptop'
 ]
 model = timm.create_model('efficientformer_l1', pretrained=True)
 model.load_state_dict(torch.load('weights/best_model_epoch_1.pt',
 map_location=torch.device('cpu')))
 model.eval()
 # Define image transformations
 test_transform = transforms.Compose([
 transforms.Resize((224, 224)),
transforms.ToTensor(),
 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
 ])
 # Function to process image and make predictions
 def process_image(image):
 input_image = test_transform(image).unsqueeze(0)
 with torch.no_grad():
 output = model(input_image)
 predicted_class = torch.argmax(output, dim=1).item()
 confidence = F.softmax(output, dim=1)[0][predicted_class].item()
 predicted_class_name = class_names[predicted_class]
 return predicted_class_name, confidence, image
 # Function to encode image as base64
 def encode_image(image):
 buffered = BytesIO()
 image.save(buffered, format="JPEG")
 encoded_img = base64.b64encode(buffered.getvalue()).decode('utf-8')
 return encoded_img
 # Define Flask routes
 @app.route('/', methods=['GET', 'POST'])
 def index():
 if request.method == 'POST':
 if 'file' not in request.files:
 return render_template('index.html', message='No file part')
 file = request.files['file']
 if file.filename == '':
 return render_template('index.html', message='No selected file')
 if file:
 image = Image.open(file).convert('RGB')
 predicted_class, confidence, original_image = process_image(image)
 encoded_image = encode_image(original_image)
 return render_template('index.html', predicted_class=predicted_class,
 confidence=confidence, encoded_image=encoded_image)
 return render_template('index.html')
 @app.route('/recognize_webcam', methods=['POST'])
 def recognize_webcam():
 data = request.json
 if 'image' in data:
 image_data = data['image']
 image = Image.open(BytesIO(base64.b64decode(image_data))).convert('RGB')
 predicted_class, confidence, _ = process_image(image)
 return jsonify(predicted_class=predicted_class, confidence=confidence)
return jsonify(error='No image data provided'), 400
 if __name__ == '__main__':
 app.run(debug=True, port=5001)
